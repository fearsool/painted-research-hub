import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import whois
import time

# --- AYARLAR ---
# 1. GeniÅŸ Kategori (Otomatik yÃ¼zlerce sayfa bulur)
CATEGORY_URL = "https://en.wikipedia.org/wiki/Category:Dog_organizations"

# 2. Senin Ã–zel Hedeflerin (Zenbuku ve Petsem iÃ§in kritik sayfalar)
TARGET_PAGES = [
    "https://en.wikipedia.org/wiki/Hairdressing",
    "https://en.wikipedia.org/wiki/Pet_sitting",
    "https://en.wikipedia.org/wiki/Animal_boarding"
]

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
OUTPUT_FILE = "final_listem.txt"

def get_sub_pages(category_url):
    print(f"ğŸ“‚ Kategori taranÄ±yor: {category_url}")
    try:
        response = requests.get(category_url, headers=HEADERS)
        soup = BeautifulSoup(response.text, 'html.parser')
        links = ["https://en.wikipedia.org" + a['href'] for a in soup.select(".mw-category a") if 'href' in a.attrs]
        return links
    except Exception as e:
        print(f"âŒ Kategori Ã§ekilirken hata: {e}")
        return []

def check_if_buyable(domain_name):
    parts = domain_name.split(".")
    main_domain = ".".join(parts[-2:]) if len(parts) > 1 else domain_name
    try:
        w = whois.whois(main_domain)
        if not w.domain_name: return True, main_domain
        return False, main_domain
    except Exception:
        return True, main_domain

# --- BAÅLANGIÃ‡ ---
print("ğŸš€ Hibrit Otomasyon BaÅŸlatÄ±ldÄ±...")

# Kategori sayfalarÄ±nÄ± ve Ã¶zel sayfalarÄ± birleÅŸtiriyoruz
all_target_urls = list(set(get_sub_pages(CATEGORY_URL) + TARGET_PAGES))
print(f"ğŸ¯ Toplam {len(all_target_urls)} farklÄ± sayfa Ã¼zerinde tarama yapÄ±lacak.\n")

for page_url in all_target_urls:
    print(f"ğŸ” Ä°nceleniyor: {page_url.split('/')[-1]}")
    try:
        response = requests.get(page_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        external_links = soup.find_all('a', href=True, class_="external text")

        for link in external_links:
            href = link['href']
            if href.startswith('http'):
                domain = urlparse(href).netloc
                if not domain: continue
                
                try:
                    # Siteye eriÅŸim testi
                    requests.get(href, headers=HEADERS, timeout=5)
                except Exception:
                    # EriÅŸim yoksa WHOIS kontrolÃ¼
                    is_available, root_domain = check_if_buyable(domain)
                    if is_available:
                        print(f"ğŸŒŸ ADAY BULDUM: {root_domain}")
                        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
                            f.write(f"Domain: {root_domain} | Kaynak: {page_url}\n")
                    else:
                        print(f"âš ï¸  Dolu: {root_domain}")
                time.sleep(0.3)
    except Exception as e:
        print(f"âŒ Sayfa hatasÄ±: {e}")

print(f"\nâœ… Tarama tamam! SonuÃ§lar '{OUTPUT_FILE}' dosyasÄ±nda.")